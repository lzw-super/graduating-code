{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellow world\n"
     ]
    }
   ],
   "source": [
    "import torch  \n",
    "print('hellow world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n",
      "Help on function is_available in module torch.cuda:\n",
      "\n",
      "is_available() -> bool\n",
      "    Returns a bool indicating if CUDA is currently available.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dir(torch.cuda.is_available)) \n",
    "help(torch.cuda.is_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module torch.utils.data.dataset in torch.utils.data:\n",
      "\n",
      "NAME\n",
      "    torch.utils.data.dataset\n",
      "\n",
      "CLASSES\n",
      "    typing.Generic(builtins.object)\n",
      "        Dataset\n",
      "            ConcatDataset\n",
      "            IterableDataset\n",
      "                ChainDataset\n",
      "            Subset\n",
      "            TensorDataset\n",
      "    \n",
      "    class ChainDataset(IterableDataset)\n",
      "     |  ChainDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  Dataset for chaining multiple :class:`IterableDataset` s.\n",
      "     |  \n",
      "     |  This class is useful to assemble different existing dataset streams. The\n",
      "     |  chaining operation is done on-the-fly, so concatenating large-scale\n",
      "     |  datasets with this class will be efficient.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      datasets (iterable of IterableDataset): datasets to be chained together\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ChainDataset\n",
      "     |      IterableDataset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, datasets: Iterable[torch.utils.data.dataset.Dataset]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from IterableDataset:\n",
      "     |  \n",
      "     |  __add__(self, other: torch.utils.data.dataset.Dataset[+T_co])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IterableDataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __getitem__(self, index) -> +T_co\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class ConcatDataset(Dataset)\n",
      "     |  ConcatDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  Dataset as a concatenation of multiple datasets.\n",
      "     |  \n",
      "     |  This class is useful to assemble different existing datasets.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      datasets (sequence): List of datasets to be concatenated\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ConcatDataset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, datasets: Iterable[torch.utils.data.dataset.Dataset]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  cumsum(sequence)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  cummulative_sizes\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'cumulative_sizes': typing.List[int], 'datasets': t...\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Dataset(typing.Generic)\n",
      "     |  Dataset(*args, **kwds)\n",
      "     |  \n",
      "     |  An abstract class representing a :class:`Dataset`.\n",
      "     |  \n",
      "     |  All datasets that represent a map from keys to data samples should subclass\n",
      "     |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      "     |  data sample for a given key. Subclasses could also optionally overwrite\n",
      "     |  :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      "     |  :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      "     |  of :class:`~torch.utils.data.DataLoader`.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |    :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      "     |    sampler that yields integral indices.  To make it work with a map-style\n",
      "     |    dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  __getitem__(self, index) -> +T_co\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class IterableDataset(Dataset)\n",
      "     |  IterableDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  An iterable Dataset.\n",
      "     |  \n",
      "     |  All datasets that represent an iterable of data samples should subclass it.\n",
      "     |  Such form of datasets is particularly useful when data come from a stream.\n",
      "     |  \n",
      "     |  All subclasses should overwrite :meth:`__iter__`, which would return an\n",
      "     |  iterator of samples in this dataset.\n",
      "     |  \n",
      "     |  When a subclass is used with :class:`~torch.utils.data.DataLoader`, each\n",
      "     |  item in the dataset will be yielded from the :class:`~torch.utils.data.DataLoader`\n",
      "     |  iterator. When :attr:`num_workers > 0`, each worker process will have a\n",
      "     |  different copy of the dataset object, so it is often desired to configure\n",
      "     |  each copy independently to avoid having duplicate data returned from the\n",
      "     |  workers. :func:`~torch.utils.data.get_worker_info`, when called in a worker\n",
      "     |  process, returns information about the worker. It can be used in either the\n",
      "     |  dataset's :meth:`__iter__` method or the :class:`~torch.utils.data.DataLoader` 's\n",
      "     |  :attr:`worker_init_fn` option to modify each copy's behavior.\n",
      "     |  \n",
      "     |  Example 1: splitting workload across all workers in :meth:`__iter__`::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_DATALOADER)\n",
      "     |      >>> # xdoctest: +SKIP(\"Fails on MacOS12\")\n",
      "     |      >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n",
      "     |      ...     def __init__(self, start, end):\n",
      "     |      ...         super(MyIterableDataset).__init__()\n",
      "     |      ...         assert end > start, \"this example code only works with end >= start\"\n",
      "     |      ...         self.start = start\n",
      "     |      ...         self.end = end\n",
      "     |      ...\n",
      "     |      ...     def __iter__(self):\n",
      "     |      ...         worker_info = torch.utils.data.get_worker_info()\n",
      "     |      ...         if worker_info is None:  # single-process data loading, return the full iterator\n",
      "     |      ...             iter_start = self.start\n",
      "     |      ...             iter_end = self.end\n",
      "     |      ...         else:  # in a worker process\n",
      "     |      ...             # split workload\n",
      "     |      ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
      "     |      ...             worker_id = worker_info.id\n",
      "     |      ...             iter_start = self.start + worker_id * per_worker\n",
      "     |      ...             iter_end = min(iter_start + per_worker, self.end)\n",
      "     |      ...         return iter(range(iter_start, iter_end))\n",
      "     |      ...\n",
      "     |      >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n",
      "     |      >>> ds = MyIterableDataset(start=3, end=7)\n",
      "     |  \n",
      "     |      >>> # Single-process loading\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n",
      "     |      [tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n",
      "     |  \n",
      "     |      >>> # xdoctest: +REQUIRES(POSIX)\n",
      "     |      >>> # Mult-process loading with two worker processes\n",
      "     |      >>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non deterministic\")\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n",
      "     |      [tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n",
      "     |  \n",
      "     |      >>> # With even more workers\n",
      "     |      >>> # xdoctest: +IGNORE_WANT(\"non deterministic\")\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=12)))\n",
      "     |      [tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n",
      "     |  \n",
      "     |  Example 2: splitting workload across all workers using :attr:`worker_init_fn`::\n",
      "     |  \n",
      "     |      >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_DATALOADER)\n",
      "     |      >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n",
      "     |      ...     def __init__(self, start, end):\n",
      "     |      ...         super(MyIterableDataset).__init__()\n",
      "     |      ...         assert end > start, \"this example code only works with end >= start\"\n",
      "     |      ...         self.start = start\n",
      "     |      ...         self.end = end\n",
      "     |      ...\n",
      "     |      ...     def __iter__(self):\n",
      "     |      ...         return iter(range(self.start, self.end))\n",
      "     |      ...\n",
      "     |      >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n",
      "     |      >>> ds = MyIterableDataset(start=3, end=7)\n",
      "     |  \n",
      "     |      >>> # Single-process loading\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n",
      "     |      [3, 4, 5, 6]\n",
      "     |      >>>\n",
      "     |      >>> # Directly doing multi-process loading yields duplicate data\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n",
      "     |      [3, 3, 4, 4, 5, 5, 6, 6]\n",
      "     |  \n",
      "     |      >>> # Define a `worker_init_fn` that configures each dataset copy differently\n",
      "     |      >>> def worker_init_fn(worker_id):\n",
      "     |      ...     worker_info = torch.utils.data.get_worker_info()\n",
      "     |      ...     dataset = worker_info.dataset  # the dataset copy in this worker process\n",
      "     |      ...     overall_start = dataset.start\n",
      "     |      ...     overall_end = dataset.end\n",
      "     |      ...     # configure the dataset to only process the split workload\n",
      "     |      ...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
      "     |      ...     worker_id = worker_info.id\n",
      "     |      ...     dataset.start = overall_start + worker_id * per_worker\n",
      "     |      ...     dataset.end = min(dataset.start + per_worker, overall_end)\n",
      "     |      ...\n",
      "     |  \n",
      "     |      >>> # Mult-process loading with the custom `worker_init_fn`\n",
      "     |      >>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n",
      "     |      [3, 5, 4, 6]\n",
      "     |  \n",
      "     |      >>> # With even more workers\n",
      "     |      >>> print(list(torch.utils.data.DataLoader(ds, num_workers=12, worker_init_fn=worker_init_fn)))\n",
      "     |      [3, 4, 5, 6]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IterableDataset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __add__(self, other: torch.utils.data.dataset.Dataset[+T_co])\n",
      "     |  \n",
      "     |  __iter__(self) -> Iterator[+T_co]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __getitem__(self, index) -> +T_co\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Subset(Dataset)\n",
      "     |  Subset(*args, **kwds)\n",
      "     |  \n",
      "     |  Subset of a dataset at specified indices.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      dataset (Dataset): The whole Dataset\n",
      "     |      indices (sequence): Indices in the whole set selected for subset\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Subset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+T_co], indices: Sequence[int]) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'dataset': torch.utils.data.dataset.Dataset[+T_co],...\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[+T_co],)\n",
      "     |  \n",
      "     |  __parameters__ = (+T_co,)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class TensorDataset(Dataset)\n",
      "     |  TensorDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  Dataset wrapping tensors.\n",
      "     |  \n",
      "     |  Each sample will be retrieved by indexing tensors along the first dimension.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      *tensors (Tensor): tensors that have the same size of the first dimension.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TensorDataset\n",
      "     |      Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __init__(self, *tensors: torch.Tensor) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'tensors': typing.Tuple[torch.Tensor, ...]}\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset[typing.Tuple[torch....\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n",
      "FUNCTIONS\n",
      "    random_split(dataset: torch.utils.data.dataset.Dataset[~T], lengths: Sequence[Union[int, float]], generator: Union[torch._C.Generator, NoneType] = <torch._C.Generator object at 0x0000023F0DB44070>) -> List[torch.utils.data.dataset.Subset[~T]]\n",
      "        Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
      "        \n",
      "        If a list of fractions that sum up to 1 is given,\n",
      "        the lengths will be computed automatically as\n",
      "        floor(frac * len(dataset)) for each fraction provided.\n",
      "        \n",
      "        After computing the lengths, if there are any remainders, 1 count will be\n",
      "        distributed in round-robin fashion to the lengths\n",
      "        until there are no remainders left.\n",
      "        \n",
      "        Optionally fix the generator for reproducible results, e.g.:\n",
      "        \n",
      "        Example:\n",
      "            >>> # xdoctest: +SKIP\n",
      "            >>> generator1 = torch.Generator().manual_seed(42)\n",
      "            >>> generator2 = torch.Generator().manual_seed(42)\n",
      "            >>> random_split(range(10), [3, 7], generator=generator1)\n",
      "            >>> random_split(range(30), [0.3, 0.3, 0.4], generator=generator2)\n",
      "        \n",
      "        Args:\n",
      "            dataset (Dataset): Dataset to be split\n",
      "            lengths (sequence): lengths or fractions of splits to be produced\n",
      "            generator (Generator): Generator used for the random permutation.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Dataset', 'IterableDataset', 'TensorDataset', 'ConcatDatas...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\lenovo\\anaconda3\\envs\\pytorch2.0.1\\lib\\site-packages\\torch\\utils\\data\\dataset.py\n",
      "\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mType:\u001b[0m        module\n",
      "\u001b[1;31mString form:\u001b[0m <module 'torch.utils.data.dataset' from 'c:\\\\Users\\\\lenovo\\\\Anaconda3\\\\envs\\\\pytorch2.0.1\\\\lib\\\\site-packages\\\\torch\\\\utils\\\\data\\\\dataset.py'>\n",
      "\u001b[1;31mFile:\u001b[0m        c:\\users\\lenovo\\anaconda3\\envs\\pytorch2.0.1\\lib\\site-packages\\torch\\utils\\data\\dataset.py\n",
      "\u001b[1;31mSource:\u001b[0m     \n",
      "\u001b[1;32mimport\u001b[0m \u001b[0mbisect\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mGeneric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mIterable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mIterator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mSequence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mTuple\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mTypeVar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mUnion\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;31m# No 'default_generator' in torch/__init__.pyi\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefault_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_accumulate\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"IterableDataset\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"TensorDataset\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"ConcatDataset\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"ChainDataset\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"Subset\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"random_split\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[0mT_co\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'T_co'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovariant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'T'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGeneric\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34mr\"\"\"An abstract class representing a :class:`Dataset`.\n",
      "\n",
      "    All datasets that represent a map from keys to data samples should subclass\n",
      "    it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      "    data sample for a given key. Subclasses could also optionally overwrite\n",
      "    :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      "    :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      "    of :class:`~torch.utils.data.DataLoader`.\n",
      "\n",
      "    .. note::\n",
      "      :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      "      sampler that yields integral indices.  To make it work with a map-style\n",
      "      dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT_co\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Dataset[T_co]'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'ConcatDataset[T_co]'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;31m# No `def __len__(self)` default?\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;31m# See NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;31m# in pytorch/torch/utils/data/sampler.py\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34mr\"\"\"An iterable Dataset.\n",
      "\n",
      "    All datasets that represent an iterable of data samples should subclass it.\n",
      "    Such form of datasets is particularly useful when data come from a stream.\n",
      "\n",
      "    All subclasses should overwrite :meth:`__iter__`, which would return an\n",
      "    iterator of samples in this dataset.\n",
      "\n",
      "    When a subclass is used with :class:`~torch.utils.data.DataLoader`, each\n",
      "    item in the dataset will be yielded from the :class:`~torch.utils.data.DataLoader`\n",
      "    iterator. When :attr:`num_workers > 0`, each worker process will have a\n",
      "    different copy of the dataset object, so it is often desired to configure\n",
      "    each copy independently to avoid having duplicate data returned from the\n",
      "    workers. :func:`~torch.utils.data.get_worker_info`, when called in a worker\n",
      "    process, returns information about the worker. It can be used in either the\n",
      "    dataset's :meth:`__iter__` method or the :class:`~torch.utils.data.DataLoader` 's\n",
      "    :attr:`worker_init_fn` option to modify each copy's behavior.\n",
      "\n",
      "    Example 1: splitting workload across all workers in :meth:`__iter__`::\n",
      "\n",
      "        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_DATALOADER)\n",
      "        >>> # xdoctest: +SKIP(\"Fails on MacOS12\")\n",
      "        >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n",
      "        ...     def __init__(self, start, end):\n",
      "        ...         super(MyIterableDataset).__init__()\n",
      "        ...         assert end > start, \"this example code only works with end >= start\"\n",
      "        ...         self.start = start\n",
      "        ...         self.end = end\n",
      "        ...\n",
      "        ...     def __iter__(self):\n",
      "        ...         worker_info = torch.utils.data.get_worker_info()\n",
      "        ...         if worker_info is None:  # single-process data loading, return the full iterator\n",
      "        ...             iter_start = self.start\n",
      "        ...             iter_end = self.end\n",
      "        ...         else:  # in a worker process\n",
      "        ...             # split workload\n",
      "        ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
      "        ...             worker_id = worker_info.id\n",
      "        ...             iter_start = self.start + worker_id * per_worker\n",
      "        ...             iter_end = min(iter_start + per_worker, self.end)\n",
      "        ...         return iter(range(iter_start, iter_end))\n",
      "        ...\n",
      "        >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n",
      "        >>> ds = MyIterableDataset(start=3, end=7)\n",
      "\n",
      "        >>> # Single-process loading\n",
      "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n",
      "        [tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n",
      "\n",
      "        >>> # xdoctest: +REQUIRES(POSIX)\n",
      "        >>> # Mult-process loading with two worker processes\n",
      "        >>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
      "        >>> # xdoctest: +IGNORE_WANT(\"non deterministic\")\n",
      "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n",
      "        [tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n",
      "\n",
      "        >>> # With even more workers\n",
      "        >>> # xdoctest: +IGNORE_WANT(\"non deterministic\")\n",
      "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=12)))\n",
      "        [tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n",
      "\n",
      "    Example 2: splitting workload across all workers using :attr:`worker_init_fn`::\n",
      "\n",
      "        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_DATALOADER)\n",
      "        >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n",
      "        ...     def __init__(self, start, end):\n",
      "        ...         super(MyIterableDataset).__init__()\n",
      "        ...         assert end > start, \"this example code only works with end >= start\"\n",
      "        ...         self.start = start\n",
      "        ...         self.end = end\n",
      "        ...\n",
      "        ...     def __iter__(self):\n",
      "        ...         return iter(range(self.start, self.end))\n",
      "        ...\n",
      "        >>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n",
      "        >>> ds = MyIterableDataset(start=3, end=7)\n",
      "\n",
      "        >>> # Single-process loading\n",
      "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n",
      "        [3, 4, 5, 6]\n",
      "        >>>\n",
      "        >>> # Directly doing multi-process loading yields duplicate data\n",
      "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n",
      "        [3, 3, 4, 4, 5, 5, 6, 6]\n",
      "\n",
      "        >>> # Define a `worker_init_fn` that configures each dataset copy differently\n",
      "        >>> def worker_init_fn(worker_id):\n",
      "        ...     worker_info = torch.utils.data.get_worker_info()\n",
      "        ...     dataset = worker_info.dataset  # the dataset copy in this worker process\n",
      "        ...     overall_start = dataset.start\n",
      "        ...     overall_end = dataset.end\n",
      "        ...     # configure the dataset to only process the split workload\n",
      "        ...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n",
      "        ...     worker_id = worker_info.id\n",
      "        ...     dataset.start = overall_start + worker_id * per_worker\n",
      "        ...     dataset.end = min(dataset.start + per_worker, overall_end)\n",
      "        ...\n",
      "\n",
      "        >>> # Mult-process loading with the custom `worker_init_fn`\n",
      "        >>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
      "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n",
      "        [3, 5, 4, 6]\n",
      "\n",
      "        >>> # With even more workers\n",
      "        >>> print(list(torch.utils.data.DataLoader(ds, num_workers=12, worker_init_fn=worker_init_fn)))\n",
      "        [3, 4, 5, 6]\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mChainDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;31m# No `def __len__(self)` default? Subclasses raise `TypeError` when needed.\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;31m# See NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34mr\"\"\"Dataset wrapping tensors.\n",
      "\n",
      "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
      "\n",
      "    Args:\n",
      "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Size mismatch between tensors\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34mr\"\"\"Dataset as a concatenation of multiple datasets.\n",
      "\n",
      "    This class is useful to assemble different existing datasets.\n",
      "\n",
      "    Args:\n",
      "        datasets (sequence): List of datasets to be concatenated\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcumulative_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0ms\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'datasets should not be an empty iterable'\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ConcatDataset does not support IterableDataset\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0midx\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"absolute value of index should not exceed dataset length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mdataset_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbisect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbisect_right\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mdataset_idx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0msample_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0msample_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mcummulative_sizes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cummulative_sizes attribute is renamed to \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                      \u001b[1;34m\"cumulative_sizes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[0mChainDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34mr\"\"\"Dataset for chaining multiple :class:`IterableDataset` s.\n",
      "\n",
      "    This class is useful to assemble different existing dataset streams. The\n",
      "    chaining operation is done on-the-fly, so concatenating large-scale\n",
      "    datasets with this class will be efficient.\n",
      "\n",
      "    Args:\n",
      "        datasets (iterable of IterableDataset): datasets to be chained together\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ChainDataset only supports IterableDataset\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ChainDataset only supports IterableDataset\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[0mSubset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34mr\"\"\"\n",
      "    Subset of a dataset at specified indices.\n",
      "\n",
      "    Args:\n",
      "        dataset (Dataset): The whole Dataset\n",
      "        indices (sequence): Indices in the whole set selected for subset\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindices\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;32mdef\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                 \u001b[0mgenerator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_generator\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSubset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34mr\"\"\"\n",
      "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
      "\n",
      "    If a list of fractions that sum up to 1 is given,\n",
      "    the lengths will be computed automatically as\n",
      "    floor(frac * len(dataset)) for each fraction provided.\n",
      "\n",
      "    After computing the lengths, if there are any remainders, 1 count will be\n",
      "    distributed in round-robin fashion to the lengths\n",
      "    until there are no remainders left.\n",
      "\n",
      "    Optionally fix the generator for reproducible results, e.g.:\n",
      "\n",
      "    Example:\n",
      "        >>> # xdoctest: +SKIP\n",
      "        >>> generator1 = torch.Generator().manual_seed(42)\n",
      "        >>> generator2 = torch.Generator().manual_seed(42)\n",
      "        >>> random_split(range(10), [3, 7], generator=generator1)\n",
      "        >>> random_split(range(30), [0.3, 0.3, 0.4], generator=generator2)\n",
      "\n",
      "    Args:\n",
      "        dataset (Dataset): Dataset to be split\n",
      "        lengths (sequence): lengths or fractions of splits to be produced\n",
      "        generator (Generator): Generator used for the random permutation.\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0msubset_lengths\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrac\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mfrac\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfrac\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Fraction at index {i} is not between 0 and 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0mn_items_in_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfrac\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0msubset_lengths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_items_in_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mremainder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_lengths\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;31m# add 1 to all the lengths in round-robin fashion until the remainder is 0\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremainder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0midx_to_add_at\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[0msubset_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_to_add_at\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubset_lengths\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Length of split at index {i} is 0. \"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m                              \u001b[1;34mf\"This might result in an empty dataset.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-overload]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSubset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_accumulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(help(torch.utils.data.dataset))\n",
    "# torch.utils.data.dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ants\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "with open(r'E:\\\\test_code\\dataset2\\train\\label\\0013035.txt') as f :\n",
    "    a = f.read() \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ThroughputBenchmark',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_contextlib',\n",
       " '_crash_handler',\n",
       " '_foreach_utils',\n",
       " '_mode_utils',\n",
       " '_osp',\n",
       " '_python_dispatch',\n",
       " '_pytree',\n",
       " '_stats',\n",
       " 'backcompat',\n",
       " 'backend_registration',\n",
       " 'cmake_prefix_path',\n",
       " 'cpp_backtrace',\n",
       " 'data',\n",
       " 'disable_minidumps',\n",
       " 'dlpack',\n",
       " 'enable_minidumps',\n",
       " 'enable_minidumps_on_exceptions',\n",
       " 'get_cpp_backtrace',\n",
       " 'hooks',\n",
       " 'rename_privateuse1_backend',\n",
       " 'set_module',\n",
       " 'sys',\n",
       " 'throughput_benchmark',\n",
       " 'weak']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 512)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "img = Image.open(r'E:\\graduating design\\test_code\\dataset1\\train\\ants\\0013035.jpg') \n",
    "print(img.size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('pytorch2.0.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49e04701d364203a368e22e16c6a99a014be3a7909364c07ecf1421d6d863dd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
